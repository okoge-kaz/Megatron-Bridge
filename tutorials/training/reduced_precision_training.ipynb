{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "004cd12a",
   "metadata": {},
   "source": [
    "# Megatron-Bridge Training Tutorial: Low-Precision Training\n",
    "\n",
    "This tutorial demonstrates how to train large language models with different numerical precision formats (BF16, FP8, MXFP8, NVFP4) using Megatron-Bridge.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Prepare and tokenize training datasets\n",
    "- Train with BF16 (baseline), FP8, MXFP8, and NVFP4 precision\n",
    "- Compare performance and memory usage across formats\n",
    "- Switch precision formats with a single line of code\n",
    "\n",
    "**Prerequisites:**\n",
    "- 4 NVIDIA GB200 GPUs (Hopper can be used for BF16 and FP8)\n",
    "- Megatron-Bridge installed\n",
    "- HuggingFace account with Llama model access\n",
    "\n",
    "**Container used:**\n",
    "This tutorial uses the [NVIDIA NeMo Framework container](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo) from NGC, which includes Megatron-Bridge and all required dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f6d981",
   "metadata": {},
   "source": [
    "## Step 1: Authenticate in HuggingFace\n",
    "\n",
    "During the tutorial we need the tokenizer of the Llama 3.1 8B. You can get it from HuggingFace. For that, introduce your HuggingFace token and run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e2152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with HuggingFace to access gated models (required for Llama)\n",
    "# \n",
    "# Steps to get your token:\n",
    "# 1. Go to https://huggingface.co/settings/tokens\n",
    "# 2. Create a new token with \"read\" permissions\n",
    "# 3. Accept the Llama model license at https://huggingface.co/meta-llama/Llama-3.1-8B\n",
    "# 4. Replace MY_TOKEN below with your actual token\n",
    "!hf auth login --token MY_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b14143",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Training Dataset\n",
    "\n",
    "We'll download NVIDIA's Nemotron high-quality synthetic dataset and tokenize it for efficient training. Tokenization converts text to numerical token IDs and creates memory-mapped binary files for fast data loading during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61134af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NVIDIA Nemotron sample dataset and prepare train/test splits\n",
    "# This high-quality synthetic dataset is designed for LLM pretraining\n",
    "import json\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "def save_jsonl(dataset, jsonl_path, text_column=\"text\"):\n",
    "    \"\"\"Save HuggingFace dataset to JSONL format (one JSON object per line)\"\"\"\n",
    "    with open(jsonl_path, \"w\") as f:\n",
    "        for sample in dataset:\n",
    "            f.write(json.dumps({\"text\": sample[text_column]}) + \"\\n\")\n",
    "\n",
    "# Load NVIDIA Nemotron high-quality synthetic pretraining dataset\n",
    "print(\"Loading dataset from HuggingFace...\")\n",
    "dataset = load_dataset(\"nvidia/Nemotron-Pretraining-Dataset-sample\", \"Nemotron-CC-High-Quality-Synthetic\")\n",
    "dataset = dataset[\"train\"].shuffle()  # Shuffle for better training dynamics\n",
    "\n",
    "# Split into 80% train, 20% test\n",
    "print(f\"Splitting dataset ({len(dataset)} samples total)...\")\n",
    "train_test = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test[\"train\"]\n",
    "test_dataset = train_test[\"test\"]\n",
    "\n",
    "# Save as JSONL files (required format for Megatron tokenization)\n",
    "print(\"Saving datasets as JSONL files...\")\n",
    "os.makedirs(\"data/raw\", exist_ok=True)\n",
    "save_jsonl(train_dataset, \"data/raw/train.jsonl\")\n",
    "save_jsonl(test_dataset, \"data/raw/test.jsonl\")\n",
    "print(f\"âœ“ Saved {len(train_dataset)} train and {len(test_dataset)} test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1ab6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Tokenize text data using Megatron-LM's preprocessing tool\n",
    "#\n",
    "# Why tokenize ahead of time?\n",
    "# - Converts text to token IDs once (not every epoch)\n",
    "# - Creates memory-mapped binary files for fast random access\n",
    "# - Enables efficient data loading during training\n",
    "# - Saves CPU time during training\n",
    "#\n",
    "# Output files created:\n",
    "# - train_text_document.bin (document/sequence data)\n",
    "# - train_text_document.idx (document/sequence metadata)\n",
    "\n",
    "MEGATRON_LM_PATH=/opt/megatron-lm/\n",
    "\n",
    "echo \"Tokenizing training data...\"\n",
    "python3 $MEGATRON_LM_PATH/tools/preprocess_data.py \\\n",
    "    --input ./data/raw/train.jsonl \\\n",
    "    --output-prefix ./data/tokenized/train \\\n",
    "    --tokenizer-type HuggingFaceTokenizer \\\n",
    "    --tokenizer-model meta-llama/Llama-3.1-8B \\\n",
    "    --log-interval 100 \\\n",
    "    --workers 4 \\\n",
    "    --append-eod  # Add end-of-document token after each sample\n",
    "\n",
    "echo \"\"\n",
    "echo \"Tokenizing test data...\"\n",
    "python3 $MEGATRON_LM_PATH/tools/preprocess_data.py \\\n",
    "    --input ./data/raw/test.jsonl \\\n",
    "    --output-prefix ./data/tokenized/test \\\n",
    "    --tokenizer-type HuggingFaceTokenizer \\\n",
    "    --tokenizer-model meta-llama/Llama-3.1-8B \\\n",
    "    --log-interval 100 \\\n",
    "    --workers 4 \\\n",
    "    --append-eod\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Tokenization complete!\"\n",
    "ls -lh ./data/tokenized/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa41beab",
   "metadata": {},
   "source": [
    "## Step 3: Explore the Default Configuration\n",
    "\n",
    "Before we start training, let's examine the default Llama 3.1 8B configuration. This shows all the settings Megatron-Bridge uses, including model architecture, training hyperparameters, and precision settings. In the examples that follow, we'll only modify the `mixed_precision` parameter to compare different formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a02edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine the default Llama 3.1 8B pretraining configuration\n",
    "# This config includes:\n",
    "# - Model architecture (32 layers, 4096 hidden size, 32 attention heads, etc.)\n",
    "# - Training hyperparameters (learning rate, batch size, optimizer settings)\n",
    "# - Mixed precision settings (default: BF16)\n",
    "# - Parallelism strategy (tensor, pipeline, data parallel)\n",
    "# - Checkpoint and logging configuration\n",
    "from pprint import pprint\n",
    "from megatron.bridge.recipes.llama.llama3 import llama31_8b_pretrain_config as pretrain_config\n",
    "\n",
    "print(\"Loading default Llama 3.1 8B configuration...\")\n",
    "cfg = pretrain_config()\n",
    "\n",
    "print(\"\\nConfiguration overview:\")\n",
    "print(f\"  Model: Llama 3.1 8B\")\n",
    "print(f\"  Default precision: {cfg.mixed_precision}\")\n",
    "print(f\"  Training iterations: {cfg.train.train_iters}\")\n",
    "print(f\"  Micro batch size: {cfg.train.micro_batch_size}\")\n",
    "print(f\"  Global batch size: {cfg.train.global_batch_size}\")\n",
    "print(\"\\nFull configuration:\")\n",
    "pprint(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878cfb2a",
   "metadata": {},
   "source": [
    "## Understanding Numerical Precision Formats\n",
    "\n",
    "**Mixed precision training** uses lower-precision number formats to accelerate training and reduce memory usage while maintaining model quality.\n",
    "\n",
    "### Why Use Lower Precision?\n",
    "- âš¡ **Faster computation**: Modern GPUs have specialized hardware for low-precision math\n",
    "- ðŸ’¾ **Less memory**: Smaller numbers = more data fits in GPU memory\n",
    "- ðŸš€ **Larger models/batches**: Use saved memory for bigger models or batch sizes\n",
    "- âœ… **Maintained quality**: Careful format design preserves training convergence\n",
    "\n",
    "### Available Formats in Megatron-Bridge:\n",
    "- **BF16** (16-bit): Industry standard, proven stability\n",
    "- **FP8** (8-bit): Faster training on H100+, hybrid E4M3/E5M2 formats\n",
    "- **MXFP8** (8-bit): Enhanced FP8 stability through microscaling\n",
    "- **NVFP4** (4-bit): Maximum efficiency for Blackwell GPUs\n",
    "\n",
    "This tutorial demonstrates how to switch between these formats with a single line of code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b8afc8",
   "metadata": {},
   "source": [
    "## Training with Different Precision Formats\n",
    "\n",
    "The configuration above shows all training settings. In the following examples, we'll modify only the `mixed_precision` parameter to compare BF16, FP8, MXFP8 and NVFP4 training while keeping everything else constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12deaa6d",
   "metadata": {},
   "source": [
    "## Example 1: BF16 Training (Baseline)\n",
    "\n",
    "**BF16** is the default format for training workloads. It offers:\n",
    "- Proven numerical stability\n",
    "- Faster than FP32 on modern GPUs\n",
    "- No special tuning required\n",
    "\n",
    "The training script below uses BF16 by default (no explicit `mixed_precision` setting needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6da3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile llama_3.1_8b_default.py\n",
    "from pprint import pprint\n",
    "from megatron.bridge.recipes.llama.llama3 import llama31_8b_pretrain_config as pretrain_config\n",
    "from megatron.bridge.training.gpt_step import forward_step\n",
    "from megatron.bridge.training.pretrain import pretrain\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load default Llama 3.1 8B config with custom training parameters\n",
    "    cfg = pretrain_config(\n",
    "        train_iters=10,              # Number of training iterations\n",
    "        micro_batch_size=2,          # Batch size per GPU\n",
    "        global_batch_size=128,       # Total batch size across all GPUs\n",
    "        lr_warmup_iters=10,          # Learning rate warmup iterations\n",
    "        lr_decay_iters=20,           # Learning rate decay iterations\n",
    "        context_parallel_size=1      # Context parallelism (1 = disabled)\n",
    "    )\n",
    "    \n",
    "    # Configure dataset paths (train, validation, test)\n",
    "    cfg.dataset.split = None\n",
    "    cfg.dataset.blend_per_split = [\n",
    "        [[\"./data/tokenized/train_text_document\"], None],\n",
    "        [[\"./data/tokenized/test_text_document\"], None],\n",
    "        [[\"./data/tokenized/test_text_document\"], None]]\n",
    "    \n",
    "    # Logging and checkpoint settings\n",
    "    cfg.logger.log_interval = 1         # Log every iteration\n",
    "    cfg.checkpoint.save = None          # Don't save checkpoints (demo only)\n",
    "    cfg.dataset.num_workers = 2         # Data loading workers\n",
    "    cfg.train.eval_iters = 0            # Skip evaluation (faster demo)\n",
    "    \n",
    "    # the BF16 format is set by default\n",
    "    # cfg.mixed_precision = \"bf16_mixed\"\n",
    "    \n",
    "    # Start training\n",
    "    pretrain(cfg, forward_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc125e51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run BF16 training on 4 GPUs\n",
    "!torchrun --standalone --nproc-per-node 4 llama_3.1_8b_default.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8483bf-6a80-44d6-842b-4cd67cf9db1c",
   "metadata": {},
   "source": [
    "With 4 GB200 you should see performance similar to following:\n",
    "```\n",
    "Step Time : 11.65s GPU utilization: 1158.1TFLOP/s/GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08c9947",
   "metadata": {},
   "source": [
    "## Example 2: FP8 Training (H100+ Only)\n",
    "\n",
    "**FP8** offers training speedup for the Hopper and Blackwell GPUs compared to BF16.\n",
    "\n",
    "The script below adds one line to enable FP8: `cfg.mixed_precision = \"bf16_with_fp8_current_scaling_mixed\"`\n",
    "\n",
    "This recipe uses:\n",
    "- FP8 hybrid mode (E4M3 for weights and activations, E5M2 for gradients)\n",
    "- BF16 for first and last transformer layers (stability)\n",
    "- Current scaling (per-iteration scaling factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91039685",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile llama_3.1_8b_fp8.py\n",
    "from pprint import pprint\n",
    "from megatron.bridge.recipes.llama.llama3 import llama31_8b_pretrain_config as pretrain_config\n",
    "from megatron.bridge.training.gpt_step import forward_step\n",
    "from megatron.bridge.training.pretrain import pretrain\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Same configuration as BF16 baseline\n",
    "    cfg = pretrain_config(\n",
    "        train_iters=10,\n",
    "        micro_batch_size=2,\n",
    "        global_batch_size=128,\n",
    "        lr_warmup_iters=10,\n",
    "        lr_decay_iters=20,\n",
    "        context_parallel_size=1\n",
    "    )\n",
    "    \n",
    "    # Dataset configuration (identical to BF16)\n",
    "    cfg.dataset.split = None\n",
    "    cfg.dataset.blend_per_split = [\n",
    "        [[\"./data/tokenized/train_text_document\"], None],\n",
    "        [[\"./data/tokenized/test_text_document\"], None],\n",
    "        [[\"./data/tokenized/test_text_document\"], None]]\n",
    "    \n",
    "    cfg.logger.log_interval = 1\n",
    "    cfg.checkpoint.save = None\n",
    "    cfg.dataset.num_workers = 2\n",
    "    cfg.train.eval_iters = 0 \n",
    "\n",
    "    # âš¡ ONLY CHANGE: Enable FP8 training (requires H100+ GPU)\n",
    "    # This uses hybrid FP8: E4M3 for weights, E5M2 for gradients\n",
    "    # with per-iteration (current) scaling for numerical stability\n",
    "    cfg.mixed_precision = \"bf16_with_fp8_current_scaling_mixed\"\n",
    "    \n",
    "    pretrain(cfg, forward_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0971cc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run FP8 training on 4 GPUs (requires H100+)\n",
    "!torchrun --standalone --nproc-per-node 4 llama_3.1_8b_fp8.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f08d77-7a37-4bf9-8c3b-92e7a0cd07c9",
   "metadata": {},
   "source": [
    "With 4 GB200 you should see performance similar to following:\n",
    "```\n",
    "Step Time : 8.57s GPU utilization: 1574.0TFLOP/s/GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632ec896",
   "metadata": {},
   "source": [
    "## Example 3: MXFP8 Training\n",
    "\n",
    "**MXFP8** is available from the Blackwell generation of GPUs and offers comparable speedup to FP8 with enhanced stability.\n",
    "\n",
    "The script below adds one line to enable FP8: `cfg.mixed_precision = \"bf16_with_mxfp8_mixed\"`\n",
    "\n",
    "This recipe uses:\n",
    "- FP8 E4M3 for weights, activations and gradients\n",
    "- Scaling factors computed for each block of 32 elements\n",
    "- Current scaling (per-iteration scaling factors)\n",
    "- No layers need to be kept in BF16 for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b810e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile llama_3.1_8b_mxfp8.py\n",
    "from pprint import pprint\n",
    "from megatron.bridge.recipes.llama.llama3 import llama31_8b_pretrain_config as pretrain_config\n",
    "from megatron.bridge.training.gpt_step import forward_step\n",
    "from megatron.bridge.training.pretrain import pretrain\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Same configuration as BF16 baseline\n",
    "    cfg = pretrain_config(\n",
    "        train_iters=10,\n",
    "        micro_batch_size=2,\n",
    "        global_batch_size=128,\n",
    "        lr_warmup_iters=10,\n",
    "        lr_decay_iters=20,\n",
    "        context_parallel_size=1\n",
    "    )\n",
    "    \n",
    "    # Dataset configuration (identical to BF16)\n",
    "    cfg.dataset.split = None\n",
    "    cfg.dataset.blend_per_split = [\n",
    "        [[\"./data/tokenized/train_text_document\"], None],\n",
    "        [[\"./data/tokenized/test_text_document\"], None],\n",
    "        [[\"./data/tokenized/test_text_document\"], None]]\n",
    "    \n",
    "    cfg.logger.log_interval = 1\n",
    "    cfg.checkpoint.save = None\n",
    "    cfg.dataset.num_workers = 2\n",
    "    cfg.train.eval_iters = 0\n",
    "    \n",
    "    # âš¡ ONLY CHANGE: Enable MXFP8 (Microscaling FP8) training\n",
    "    # Uses block-based scaling (32 elements/block) for better stability\n",
    "    # Optimized for Blackwell (B200+) GPUs\n",
    "    cfg.mixed_precision = \"bf16_with_mxfp8_mixed\"\n",
    "    \n",
    "    pretrain(cfg, forward_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c4154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MXFP8 training on 4 GPUs\n",
    "# Expected: ~8-9s per step, ~1500 TFLOP/s/GPU (~30% faster than BF16)\n",
    "!torchrun --standalone --nproc-per-node 4 llama_3.1_8b_mxfp8.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f304e1d",
   "metadata": {},
   "source": [
    "With 4 GB200 you should see performance similar to following:\n",
    "```\n",
    "Step Time : 8.63s GPU utilization: 1562.8TFLOP/s/GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87816d6f",
   "metadata": {},
   "source": [
    "## Example 4: NVFP4 Training\n",
    "\n",
    "**NVFP4** is available from the Blackwell generation of GPUs and offers further speedups compared to FP8 and MXFP8 while maintaining accuracy.\n",
    "\n",
    "The script below adds one line to enable NVFP4: `cfg.mixed_precision = \"bf16_with_nvfp4_mixed()\"`\n",
    "\n",
    "This recipe uses:\n",
    "- FP4 (E2M1) values with two-level scaling\n",
    "- groups of 16 values share a high-precision FP8 (E4M3) scale plus a per-tensor FP32 scale\n",
    "- further techniques such as Randomized Hadamard Transform, 2D block-scaling for the weights, and stochastic rounding for the gradients\n",
    "\n",
    "We recommend to keep the last 15% of the layers in BF16 precision for better stability. As a result, for the Llama 3.1 8B model we keep 4 out of the 32 layers in BF16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca9e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile llama_3.1_8b_fp4.py\n",
    "from pprint import pprint\n",
    "from megatron.bridge.recipes.llama.llama3 import llama31_8b_pretrain_config as pretrain_config\n",
    "from megatron.bridge.training.gpt_step import forward_step\n",
    "from megatron.bridge.training.pretrain import pretrain\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Same configuration as BF16 baseline\n",
    "    cfg = pretrain_config(\n",
    "        train_iters=10,\n",
    "        micro_batch_size=2,\n",
    "        global_batch_size=128,\n",
    "        lr_warmup_iters=10,\n",
    "        lr_decay_iters=20,\n",
    "        context_parallel_size=1,\n",
    "    )\n",
    "    \n",
    "    # Dataset configuration (identical to BF16)\n",
    "    cfg.dataset.split = None\n",
    "    cfg.dataset.blend_per_split = [\n",
    "        [[\"./data/tokenized/train_text_document\"], None],\n",
    "        [[\"./data/tokenized/test_text_document\"], None],\n",
    "        [[\"./data/tokenized/test_text_document\"], None]]\n",
    "    \n",
    "    cfg.logger.log_interval = 1\n",
    "    cfg.checkpoint.save = None\n",
    "    cfg.dataset.num_workers = 2\n",
    "    cfg.train.eval_iters = 0\n",
    "    \n",
    "    # âš ï¸ Enable NVFP4 training keeping the last 4 layers in BF16\n",
    "    from megatron.bridge.training.mixed_precision import MixedPrecisionConfig, bf16_with_nvfp4_mixed\n",
    "    def bf16_with_nvfp4_mixed_4_layers_bf16() -> MixedPrecisionConfig:\n",
    "        cfg = bf16_with_nvfp4_mixed()\n",
    "        cfg.first_last_layers_bf16 = True\n",
    "        cfg.num_layers_at_end_in_bf16 = 4\n",
    "        return cfg\n",
    "    cfg.mixed_precision = bf16_with_nvfp4_mixed_4_layers_bf16()\n",
    "    pretrain(cfg, forward_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109594a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run NVFP4 training on 4 GPUs\n",
    "!torchrun --standalone --nproc-per-node 4 llama_3.1_8b_fp4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9d3be0-d3d8-4e1d-9a0d-8d4f07efa5fd",
   "metadata": {},
   "source": [
    "With 4 GB200 you should see performance similar to following:\n",
    "```\n",
    "Step Time : 7.47s GPU utilization: 1805.6TFLOP/s/GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f391c5b8",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "## Quick Guide\n",
    "\n",
    "**Older GPUs:** BF16 (default)\n",
    "```python\n",
    "cfg.mixed_precision = \"bf16_mixed\"  # or omit\n",
    "```\n",
    "\n",
    "**H100+, higher throughput:** FP8\n",
    "```python\n",
    "cfg.mixed_precision = \"bf16_with_fp8_current_scaling_mixed\"\n",
    "```\n",
    "\n",
    "**B200+, better stability:** MXFP8\n",
    "```python\n",
    "cfg.mixed_precision = \"bf16_with_mxfp8_mixed\"\n",
    "```\n",
    "\n",
    "**B200+, highest throughput:** NVFP4\n",
    "```python\n",
    "cfg.mixed_precision = \"bf16_with_nvfp4_mixed\"\n",
    "```\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Start with BF16** - establish a stable baseline first\n",
    "2. **Validate with lower precision** - Compare convergence, loss curves, final model quality\n",
    "3. **Hardware matters** - FP8 beneficial on H100+, MXFP8/NVFP4 on B200+\n",
    "4. **One line config** - Simple to switch: `cfg.mixed_precision = \"recipe_name\"`\n",
    "5. **Memory savings** - Use lower precision to increase batch size or model size\n",
    "\n",
    "\n",
    "## Expected Performance (Llama 3.1 8B on 4x GB200 GPUs)\n",
    "\n",
    "| Precision  | Step Time | GPU Utilization  | Speedup vs BF16 | Hardware |\n",
    "|------------|-----------|------------------|-----------------|----------|\n",
    "| **BF16**   | ~11.65s   | 1158 TFLOP/s/GPU |         -       |  Any GPU |\n",
    "| **FP8**    | ~8.57s    | 1574 TFLOP/s/GPU |       1.36x     |   H100+  |\n",
    "| **MXFP8**  | ~8.63s    | 1562 TFLOP/s/GPU |       1.35x     |   B200+  |\n",
    "| **NVFP4**  | ~7.47s    | 1805 TFLOP/s/GPU |       1.56x     |   B200+  |\n",
    "\n",
    "*Note: Performance numbers are approximate and will vary based on model size, batch size, and hardware configuration.*\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Pretraining Large Language Models with NVFP4](https://arxiv.org/abs/2509.25149)\n",
    "- [NVFP4 blogpost](https://developer.nvidia.com/blog/introducing-nvfp4-for-efficient-and-accurate-low-precision-inference/\n",
    ")\n",
    "- [Mixed Precision Training Guide](https://docs.nvidia.com/nemo/megatron-bridge/latest/training/mixed-precision.html)\n",
    "- [Using FP8 and FP4 with Transformer Engine](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html)\n",
    "\n",
    "Happy training! ðŸš€\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# ==============================================================================
# Template YAML Configuration for GPT-based Model Training
# ==============================================================================
#
# This template shows all ConfigContainer sections you can override.
# Most fields are commented out - uncomment and modify as needed.
#
# YAML structure mirrors ConfigContainer fields:
# - data: GPTDatasetConfig | FinetuningDatasetConfig | DatasetProvider
# - train: TrainingConfig
# - model: Model Provider (GPTModelProvider, etc.)
# - optimizer: OptimizerConfig
# - scheduler: SchedulerConfig
# - checkpoint: CheckpointConfig
# - logger: LoggerConfig
# - peft: PEFT (for finetuning with LoRA/DoRA)
# - ddp: DistributedDataParallelConfig
# - dist: DistributedInitConfig
# - rng: RNGConfig
#
# ==============================================================================

# Data configuration
# data:
#   # For pretraining (GPTDatasetConfig):
#   data_path: /path/to/dataset
#   sequence_length: 4096
#   
#   # For finetuning (FinetuningDatasetConfig):
#   # data_path: /path/to/dataset.jsonl
#   # seq_length: 4096

# Training configuration
# train:
#   train_iters: 1000
#   global_batch_size: 256
#   micro_batch_size: 2
#   eval_iters: 10
#   eval_interval: 100

# Model configuration
# model:
#   seq_length: 4096
#   tensor_model_parallel_size: 1
#   pipeline_model_parallel_size: 1
#   context_parallel_size: 1

# Optimizer configuration
# optimizer:
#   lr: 0.0003
#   min_lr: 0.00003
#   weight_decay: 0.1

# Learning rate scheduler
# scheduler:
#   lr_warmup_iters: 100
#   lr_decay_style: cosine

# Checkpoint configuration
# checkpoint:
#   save: ./checkpoints/my_model
#   save_interval: 100
#   # For resuming:
#   # load: ./checkpoints/my_model/
#   # For finetuning:
#   # pretrained_checkpoint: /path/to/pretrained/checkpoint

# LoRA/PEFT configuration (finetuning only)
# peft:
#   _target_: megatron.bridge.peft.lora.LoRA
#   dim: 8      # LoRA rank
#   alpha: 16   # LoRA alpha scaling

# Logging configuration
# logger:
#   log_interval: 10
#   tensorboard_dir: ./logs
#   # wandb_project: my_project
#   # wandb_entity: my_team

# Random seed
# rng:
#   seed: 1234

# Distributed Data Parallel settings
# ddp:
#   grad_reduce_in_fp32: true

# Distributed initialization
# dist:
#   distributed_timeout_minutes: 15

